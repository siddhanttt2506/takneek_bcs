{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use this file to run the modified LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install peft bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftConfig, PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "base_model_name = \"qu-bit/SuperLLM\"\n",
    "adapter_model_name = \"ashishu23/model1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "model = PeftModel.from_pretrained(model, adapter_model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(model, tokenizer, question):\n",
    "    # Tokenize the question and move the tensors to CUDA\n",
    "    inputs = tokenizer(question, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate response with adjusted parameters for longer output\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=1500,       # Increase this number for longer responses\n",
    "        temperature=0.1,      # Adjusts randomness; lower is more focused\n",
    "        top_p=0.9,            # Nucleus sampling to keep coherent outputs\n",
    "        top_k=50,             # Limits the number of highest probability tokens\n",
    "        do_sample=True,       # Enables sampling to get varied outputs\n",
    "        num_return_sequences=1 # Number of sequences to generate\n",
    "    )\n",
    "\n",
    "    # Decode the output and skip special tokens\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different queries can be asked by changing the question in the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask the question\n",
    "question = \"Who is Narendra Modi? And give all the details of his family,friends and life.\"\n",
    "response = ask_question(model, tokenizer, question)\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
